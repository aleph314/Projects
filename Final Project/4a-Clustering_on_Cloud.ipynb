{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Box Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Notebooks\n",
    "\n",
    "- [Data Ingestion and Cleaning](1-Data_Ingestion_and_Cleaning.ipynb)\n",
    "- [EDA](2-EDA.ipynb)\n",
    "- [Apriori](3-Apriori.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I use k-means to cluster the annualities. I try four different sets of features: the patterns obtained from apriori in the previous notebooks and some statistics computed considering each annuality as a time series (e.g. mean, standard deviation...).\n",
    "\n",
    "The clusterings are done on dataset of over 5GBs, so in order to be able to execute them I ran this notebook on [Cloud Datalab](https://cloud.google.com/datalab/), retrieving data from [Google Cloud Storage Buckets](https://cloud.google.com/storage/docs/json_api/v1/buckets) by means of the `datalab.storage` module that allows the integration of jupyter notebooks executed on Datalab with the Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datalab.context import Context\n",
    "import datalab.storage as storage\n",
    "from io import BytesIO\n",
    "# setting the bucket name\n",
    "sample_bucket = storage.Bucket('k2proj_ale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the file for time series' data\n",
    "remote_csv = sample_bucket.item('voucher.csv').read_from()\n",
    "# reading the file into a panda df\n",
    "vouchers = pd.read_csv(BytesIO(remote_csv))\n",
    "# setting indices and dropping columns\n",
    "vouchers.drop(['km_bin', 'km_quant'], inplace=True, axis=1)\n",
    "vouchers.set_index(['n_voucher', 'annuality'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the file for arbitrary ranges\n",
    "remote_pickle = sample_bucket.item('processed/vou_bin_full.pkl').read_from()\n",
    "# reading the file into a panda df\n",
    "vou_apriori = pd.read_pickle(BytesIO(remote_pickle))\n",
    "# setting the file for quantiles' ranges\n",
    "remote_pickle = sample_bucket.item('processed/vou_quant_full.pkl').read_from()\n",
    "# reading the file into a panda df\n",
    "vou_apriori_quant = pd.read_pickle(BytesIO(remote_pickle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before clustering the data I will normalize it.\n",
    "\n",
    "First I'll apply tf-idf to the apriori features: this will normalize the data and weight less some very common patterns (e.g. the AAA pattern which translates to \"not using the car for three days in a row\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "vou_apriori = pd.DataFrame(tfidf.fit_transform(vou_apriori.values).todense(), index=vou_apriori.index, columns=vou_apriori.columns)\n",
    "vou_apriori_quant = pd.DataFrame(tfidf.fit_transform(vou_apriori_quant.values).todense(), index=vou_apriori_quant.index, columns=vou_apriori_quant.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I'll scale the time series' data to have comparable units to the ones used for apriori features after tf-idf and I'll join the scaled features back in the apriori datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "vouchers = pd.DataFrame(sc.fit_transform(vouchers), index=vouchers.index, columns=vouchers.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vou_apriori_all = vou_apriori.copy()\n",
    "vou_apriori_all = vou_apriori_all.join(vouchers)\n",
    "vou_apriori_quant_all = vou_apriori_quant.copy()\n",
    "vou_apriori_quant_all = vou_apriori_quant_all.join(vouchers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I'll use k-means to cluster the annualities: I'm fitting four different clustering using 10 clusters, a number I determined by running the algorithm on a subset of the data and looking at inertia for various number of clusters and choice of features (see this [notebook](0-10000 Clients Analysis.ipynb)).\n",
    "\n",
    "The clustering I try are:\n",
    "\n",
    "- only apriori data with arbitrary ranges\n",
    "- only apriori data with quantiles ranges determined from quantiles\n",
    "- apriori data with arbitrary ranges and time series' data\n",
    "- apriori data with quantiles ranges determined from quantiles and time series' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Inertia](./reports/figures/inertia.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans()\n",
    "km_quant = KMeans()\n",
    "km_all = KMeans()\n",
    "km_quant_all = KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.set_params(n_clusters=10)\n",
    "km.fit(vou_apriori)\n",
    "km_quant.set_params(n_clusters=10)\n",
    "km_quant.fit(vou_apriori_quant)\n",
    "km_all.set_params(n_clusters=10)\n",
    "km_all.fit(vou_apriori_all)\n",
    "km_quant_all.set_params(n_clusters=10)\n",
    "km_quant_all.fit(vou_apriori_quant_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vou_apriori['label'] = km.labels_\n",
    "vou_apriori_quant['label'] = km_quant.labels_\n",
    "vou_apriori_all['label'] = km_all.labels_\n",
    "vou_apriori_quant_all['label'] = km_quant_all.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm also saving the distances from each cluster's centroid, in order to be able to find the closest annualities to them later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = pd.DataFrame(km.transform(vou_apriori.drop('label', axis=1)), \n",
    "                         index=vou_apriori.index, \n",
    "                         columns=['dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5', 'dist6', 'dist7', 'dist8', 'dist9'])\n",
    "distances_quant = pd.DataFrame(km_quant.transform(vou_apriori_quant.drop('label', axis=1)), \n",
    "                               index=vou_apriori_quant.index, \n",
    "                               columns=['dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5', 'dist6', 'dist7', 'dist8', 'dist9'])\n",
    "distances_all = pd.DataFrame(km_all.transform(vou_apriori_all.drop('label', axis=1)), \n",
    "                             index=vou_apriori_all.index, \n",
    "                             columns=['dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5', 'dist6', 'dist7', 'dist8', 'dist9'])\n",
    "distances_quant_all = pd.DataFrame(km_quant_all.transform(vou_apriori_quant_all.drop('label', axis=1)), \n",
    "                                   index=vou_apriori_quant_all.index, \n",
    "                                   columns=['dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5', 'dist6', 'dist7', 'dist8', 'dist9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vou_apriori = vou_apriori.join(distances)\n",
    "vou_apriori_quant = vou_apriori_quant.join(distances_quant)\n",
    "vou_apriori_all = vou_apriori_all.join(distances_all)\n",
    "vou_apriori_quant_all = vou_apriori_quant_all.join(distances_quant_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally I'm saving the labelled datasets to a bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bucket.item('processed/vou_bin_full_labeled.csv')\\\n",
    "             .write_to(vou_apriori[['label', 'dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5', 'dist6', 'dist7', 'dist8', 'dist9']]\\\n",
    "               .to_csv(), 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_bucket.item('processed/vou_quant_full_labeled.csv')\\\n",
    "             .write_to(vou_apriori_quant[['label', 'dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5', 'dist6', 'dist7', 'dist8', 'dist9']]\\\n",
    "               .to_csv(), 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_bucket.item('processed/vou_bin_all_full_labeled.csv')\\\n",
    "             .write_to(vou_apriori_all[['label', 'dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5', 'dist6', 'dist7', 'dist8', 'dist9']]\\\n",
    "               .to_csv(), 'text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_bucket.item('processed/vou_quant_all_full_labeled.csv')\\\n",
    "             .write_to(vou_apriori_quant_all[['label', 'dist0', 'dist1', 'dist2', 'dist3', 'dist4', 'dist5', 'dist6', 'dist7', 'dist8', 'dist9']]\\\n",
    "               .to_csv(), 'text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following Notebooks\n",
    "\n",
    "- [Clustering on Premises](4b-Clustering_on_Prem.ipynb)\n",
    "- [Interpreting Clusters](5-Interpreting_Clusters.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
